{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config Iniciais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install bitsandbytes\n",
    "!pip install accelerate\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import transformers\n",
    "import torch\n",
    "import getpass\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "login(token=\"_token_\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Meta-Llama-3-8B-instruct\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "                      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_generator = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=False,\n",
    "    max_new_tokens=512,\n",
    "    do_sample=True,\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "\n",
    "def get_response(prompt):\n",
    "  response = text_generator(prompt)\n",
    "  gen_text = response[0]['generated_text']\n",
    "  return gen_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemplos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"O que é computação quântica? desejo que a minha resposta venha em portugues\"\n",
    "llama_response = get_response(prompt)\n",
    "print(llama_response)\n",
    "\n",
    "\n",
    "# prompt = \"Qual o resultado de 7 x 6 - 42?\"\n",
    "# llama_response = get_response(prompt)\n",
    "# print(llama_response)\n",
    "\n",
    "# prompt = \"Quem foi a primeira pessoa no espaço?\"\n",
    "# llama_response = get_response(prompt)\n",
    "# print(llama_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## templates e Engenharias de Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(prompt):\n",
    "  response = text_generator(prompt)\n",
    "  gen_text = response[0]['generated_text']\n",
    "  return gen_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Quem foi a primeira pessoa no espaço?\"\n",
    "template  = \"\"\"<|system|>\n",
    "você é um assistente útil.\n",
    "\n",
    "<|user|>\n",
    "\"{}\"<|end|>\n",
    "\n",
    "<|assistant|>\n",
    "\"\"\".format(prompt)\n",
    "output = get_response(template)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Você entende português?\"\n",
    "template  = \"\"\"<|system|>\n",
    "você é um assistente útil.\n",
    "\n",
    "<|user|>\n",
    "\"{}\"<|end|>\n",
    "\n",
    "<|assistant|>\n",
    "\"\"\".format(prompt)\n",
    "output = get_response(template)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"O que é ia\" #@param {type:\"string\"}\n",
    "\n",
    "template  = \"\"\"<|system|>\n",
    "você é um assistente útil.\n",
    "\n",
    "<|user|>\n",
    "\"{}\"<|end|>\n",
    "\n",
    "<|assistant|>\n",
    "\"\"\".format(prompt)\n",
    "output = get_response(template)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explorando Engenharia de Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = \"O que é ia\" #@param {type:\"string\"}\n",
    "# prompt = \"O que é IA? responda em 1 frase\"\n",
    "prompt = \"O que é IA? responda em forma de poema\"\n",
    "sys_prompt = \"Você é um assistente virtual prestativo, responda as perguntas em português!\"\n",
    "template  = \"\"\"<|system|>\n",
    "{}<|end|>\n",
    "\n",
    "<|user|>\n",
    "\"{}\"<|end|>\n",
    "\n",
    "<|assistant|>\n",
    "\"\"\".format(sys_prompt , prompt)\n",
    "output = get_response(template)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Gere um código em python que escreva a sequência de fibonacci\"\n",
    "sys_prompt = \"Você é um programador experiente. Retorne o código requisitado e se quiser forneça explicações breves, caso convenientes\"\n",
    "template  = \"\"\"<|system|>\n",
    "{}<|end|>\n",
    "\n",
    "<|user|>\n",
    "\"{}\"<|end|>\n",
    "\n",
    "<|assistant|>\n",
    "\"\"\".format(sys_prompt , prompt)\n",
    "output = get_response(template)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formato de Mensagens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"O que é IA?\"\n",
    "\n",
    "msg = [\n",
    "    \n",
    "       {\"role\" : \"system\" , \"content\" : \"Você é um assistente virtual prestativo, responda as perguntas em português!\"},\n",
    "       {\"role\" : \"user\" , \"content\" : prompt}\n",
    "]\n",
    "\n",
    "output = get_response(msg)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Liste o nome de 10 cidades famosas da Europa\"\n",
    "\n",
    "msg = [\n",
    "    \n",
    "       {\"role\" : \"system\" , \"content\" : \"Você é um assistente virtual prestativo, responda as perguntas em português!\"},\n",
    "       {\"role\" : \"user\" , \"content\" : prompt}\n",
    "]\n",
    "\n",
    "output = get_response(msg)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantização - Bits and bytes config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - A quantização reduz a precisão dos números usados para representar os parâmetros do modelo, diminuindo o número de memória e os requisitos computacionais\n",
    "\n",
    "### - Ao invés de usar números de ponto flutuantes de 32 bits(float32), o modelo pode usar 16 , ou até 8 bits.\n",
    "\n",
    "### - Este processo pode reduzir significativamente o tamanho do modelo e acelerar a inferência sem causar uma perda substancial na precisão\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benefícios para LLMs:\n",
    "\n",
    "1 . permite rodar grandes modelos de linguagem em hardware com recursos limitados;\n",
    "\n",
    "2. Mantém um bom desempenho sem comprometer significativamente a precisão"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usando o Meta-Llama-3-8B-instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = (\"Quem foi a primeira pessoa no espaço?\")\n",
    "messages = [{\"role\" : \"user\" , \"content\" : prompt}]\n",
    "\n",
    "encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
    "model_inputs = encodeds.to(device)\n",
    "generate_ids = model.generate(\n",
    "    model_inputs, \n",
    "    max_new_tokens=1000, \n",
    "    do_sample=True, \n",
    "    pad_token_id=tokenizer.pad_token_id,  \n",
    "    eos_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "decoded = tokenizer.batch_decode(generate_ids)\n",
    "\n",
    "output = decoded[0]\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
